%
% File naaclhlt2015.tex
%

\documentclass[11pt,letterpaper]{article}
\usepackage{naaclhlt2015}
\usepackage{times}
\usepackage{latexsym}
\setlength\titlebox{6.5cm}    % Expanding the titlebox

\title{Evaluation of Twitter as semantic analysis resource}

\author{}

\date{November, 5 2014}

\begin{document}
\maketitle
\begin{abstract}
  In this paper use of Russian-speaking Twitter segment as a resource for distributional
  semantic analysis is evaluated. Though gold-standard for task of semantic relatedness
  of Russian language does not yet exist, some tweaks may be done to substitute it.  
  There are several gold-standards for English, German and other languages. 
  Translated versions of one of these, WordSim353, is applied.  It may be biased because of 
  cultural and language differencies. To overcome this, semantics extracted from Twitter
  are cross-checked with ones extracted from corpus of contemporary Russian literature.
\end{abstract}

\section{Introduction}

One of the vital goals in task of distributional semantics of Russian language is building
a gold-standard, analogous to WordSim353 \cite{finkelstein2001placing} and others. 
Such datasets are composed with use of expert knowledge. At first, researcher composes pairs
of words, either related somehow (meronymy, hyponymy, hyperonymy, antonymy, etc.), or unrelated.
Next, a group of individual experts are given the task to estimate relatedness of those pairs.
Estimations may vary, e.g. four distinct grades, or fraction from 0.0 to 1.0. Typical group size is 
about 10-15 people.

Apperently, researchers wanting to make such datasets for other languages may try to use existing
ones as basis, e.g. by simply translating them, because reengineering it from scratch would be much more
complicated. Some did try, but as far as we know there was no proper evaluation of translated word-sim
datasets.

There is strong intiution that distributional semantic and all state-of-the-art forecomings may
be applied to Russian language as well. Information Retrieval in a whole has been successfully applied
to many languages (by Google, Yandex\footnote{www.yandex.com} and others). Semantic analysis
has been developing for decades, starting with Latent Semantic Analysis \cite{landauer1998introduction}, 
Latent Semantic Indexing, finally Explicit Semantic Analysis \cite{gabrilovich2007computing}, and numerous
Wordnet and Wikipedia-based works \cite{zesch2008extracting}.

These may be broadly divided into two groups: ones based on graph models and 
measurements (e.g. Wikipedia-based), and distributional ones (LSA, LSI).

Early models, like LSA, suffered from increasing number of documents and their terms. 
The core feature of LSA, singular value decomposition, stated as the one to reduce vector-space model 
of term-document matrix, actually makes it unfeasiable because of expensive SVD operation.
Tremendous shift in field of vector-space models for natural language processing was made 
by \cite{mikolov2013efficient}, with use of simplified neural network models, Continious Bag-of-Words
and Continious Skip-gram. It was evaluated on word-similarity task. 

On the other hand, increasing popularity of social networks, and in particular Twitter,
enables users to communicate instantly, and researchers to analyse their activity.
The area is "hot ground", there are works such as elections prediction \cite{metaxas2011not}, sentiment
analysis and so on.

\subsection{Goals of this paper}

In this paper we focus on word-to-word similarity task for Russian language. The primary goal
is to evaluate applicability of Twitter stream in this NLP task. The secondary goals are
construction and validation of word similarity dataset for Russian based on WordSim353 translated 
subset, and cross-checking it with corpus of Russian literature.


\section{Input data and algorythm}

\subsection{Input data}

\begin{table}
\begin{center}
\begin{tabular}{|l|r|r|}
\hline \bf Chunk id & \bf Word count & \bf WordSim, $\rho_P$  \\ \hline
{\tt 01} & 1.2M  & 0.25\\
{\tt 01\_10} & 12.5M & 0.27 \\
{\tt 01\_20} & 22.5M & 0.28 \\
{\tt books} & 450K & 0.31 \\
\hline
\end{tabular}
\end{center}
\caption{\label{chunk-size-table} Corpus under study }
\end{table}


Twitter data is mined from Twitter streaming API. It is assumed to be random subset of actual Twitter stream.

Big picture of algorythm:
\begin{itemize}
\item fetch tweets
\item filter non-Cyrillic ones
\item stemming (each tweet as one sentence)
\item remove stopwords
\item store words in database.
\end{itemize}

All fetched data is stored in daily chunks, appox. 450k Tweets per day.

We detect Russian words by simply counting cyrillic symbols. We use Yandex Tomita Parser\footnote{https://tech.yandex.ru/tomita/}
for stemming (which also filteres out words not looking like Russian ones).

Algorythm for parsing books slightly differs:
\begin{itemize}
\item split text by sentences
\item stemming
\item remove stopwords
\item store in database
\end{itemize}

\subsection{Counting distributions}

The word distribution matrix has word-by-word structure. 
Each row is frequence distribution of word context. 
Every cell X in row Y describes number of sentences with both x and y.

Each cell in matrix is then weighted with entropy: 
$- \sum p \log p$, as stated by \cite{landauer1998introduction}.

Next we compute cosine similarity between word distributions (which is common for such task).

\section{Evaluation}

Common approach for evaluation of word semantic relatedness is to use several datasets (3-4 typically). 
Since we want to evaluate if translated one is worthwhile, it's enough to translate just one. 
We make an asdumtion that bias of translation would be more significant than the one of a dataset.

\subsection{Method of translation}

First of all, we consider only 2000 most frequent words in our Twitter corpus.  
We make list of words from WordSim353 combined set. For each word we manually 
lookup translation with dictionary \footnote{slovari.yandex.ru} and write down short-list 
of possible translations. If translation is not present in 2000 list, we remove it. 
Empty translation lists are removed. Pairs from WordSim with partial or none 
translation are also removed.

Still we managed to translate roughly 250 words out of 450, giving us 100 translated pairs.
This is the baseline for all our subsequent experiments. 

\section{Experimental results}

Дык ёпта.

\begin{table*}[t]
\begin{center}
\begin{tabular}{|llll|r|r|}
\hline \bf Pair & & & &   $m_{WordSim353} - m_{\tt books} $  \\ \hline

\hline
\end{tabular}
\end{center}
\caption{\label{top10-books} Top10 {\tt books} errors }
\end{table*}


Twitter corpus under study consists of 20 chunks since 1 to 20 August, 2014, one chunk per day.
Each chunk contains $\approx 1.3M$ words.
We also consider joined chunks, namely {\tt 01\_10}, {\tt 11\_20} containing 10 days each, and 
{\tt 01\_20} containing entire corpus.
We use Pearson correlation coefficient to estimate accuracy of our method.

First measurements of Pearson correlation for single-day chunk, witout using stopwords filtering, 
showed as low as 0.20, in contrast with 0.6 being state-of-the-art accuracy 
for this task \cite{mikolov2013efficient}.  


This low accuracy may be in result of following factors:

\begin{itemize}
\item errors in translation 
\item algorythm issues
\item corpus quality
\item size of corpus
\end{itemize}

Usage of stopwords helped us to make 0.25. 

Next obvious step was to determine if we can improve accuracy with just more data.

It gave us another 2-3\% (Table~\ref{chunk-size-table}), with 20 times larger
corpus. Apparently, enlarging it further doesnt make much sence.

Still we had to mitigate possible translation issues and corpus-related ones.

\subsection{Russian literature corpus}

To overcome translation issues we mined different corpus, using publicly 
available books of Russian contemporary authors (late 20th century - beginning
of 21th, for list of them see Appendix A). In this paper we address it 
{\tt books}.

Size of corpus was chosen empirically: we stopped adding texts when 
no significant improvement in accuracy was be noticed.

It's top was around 0.30, which is slightly better than {\tt 01\_20}, 
and much better than {\tt 01}. Inter-correlation between {\tt 01\_20} 
and {\tt books} appeared to be 0.63 (on the set of pairs
from translated WordSim).

From this point we may make some conclusions about quality of 
Twitter as lingustic resource. First of all, pretty strong correlation
with {\tt books} corpus lets us state that they have much in common, and
Twitter may be used as linguistic resource. 

On the other side, considering that {\tt books} is 50
times smaller than {\tt 01\_20}, it's closest contestant with 0.28 accuracy, 
one may conclude that literature is more information-dense linguistic resource.
Which is not surprizing.

\subsection{Language issues}

In order to validate our word translations, we consider word pairs 
and their gold-standard values. Our empirical study shows, that most
significant error in relatedness estimation was given by:




\bibliography{rutwitter-semantics}
\bibliographystyle{naaclhlt2015}

{\bf Appendix A. Texts in {\tt books} corpus}.
\label{books-app}

B. Akunin. Almaznaya kolesnitsa. 

B. Akunin. Vneklassnoe chtenie.

D. Granin. Zubr. 


\end{document}
